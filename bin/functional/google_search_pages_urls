#!/usr/bin/python3
print("""
#########################################################################################################
# This script get 2 inputs: 1-search qery, 2-pages qty, and then give you a urls of google search pages #
#########################################################################################################
""")
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
import time
import numpy as np
from selenium.webdriver.firefox.options import Options

# url = input("Enter your first pages url:\n")

query = input("Enter your search query:\n")
pages = int(input("Enter pages Qty requried: "))
print("\n")

url = "https://www.google.com/search?q=" + query.replace(" ", "+")

options = Options()
options.add_argument("--headless")
print()
browser = webdriver.Firefox(executable_path = "/home/amir/github/working/Facebook_posts_links/geckodriver", options=options)
p = 0
all_urls = []
while p < pages:
	p += 1
	print("Page# " + str(p))
	#navigates you to the url page.
	try:
		browser.get(url)
		s = BeautifulSoup(browser.page_source, "lxml")
		url = "https://www.google.com/" + s.find("a", {"id" : "pnnext", "class" : "G0iuSb"})['href']
	except:
		print("\n\nSorry we could'nt completed our url extraction, there is only " + str(len(all_urls)) + " urls extracted among " + str(pages) + " requred pages\n'n")
		break
	all_urls.append(url)
#	 print('len(all_urls):', len(all_urls))
	time.sleep(3)
print("*************************** Successfully Completed ***************************")
for urL in all_urls:
	print(urL)