#!/usr/bin/python3
from termcolor import colored
# from tabulate import tabulate
# import pandas as pd
# import requests
# from bs4 import BeautifulSoup
# url = "https://next-episode.net/uyanis-buyuk-selcuklu"
# soup = BeautifulSoup(requests.get(url).text, "lxml")
# l = [i.strip() for i in soup.find("div", {"id": "next_episode"}).text.split("\n")[3:] if i][:-1]
# if "Sorry, no info about the next episode" in "\n".join(l):
# 	print("\n" + '\n'.join(l).strip() + "\n")
# 	print("\n\nhttps://www.multipointtv.net/uyanis-buyuk-selcuklu-episode-3-english-urdu-subtitles/\n")
# 	import sys
# 	sys.exit(1)
# d = {}
# for i in l:
#     a = i.split(":")
#     d[a[0]] = a[1]
# df = pd.DataFrame(d, index=["info"]).T
# print(tabulate(df))

print()

from tabulate import tabulate
import os
import sys
import pandas as pd
import requests
from bs4 import BeautifulSoup
url = "https://www.tvmaze.com/shows/50131/uyanis-buyuk-selcuklu"
print(f"\nThis information is collected from:  {url}\n\n")
soup = BeautifulSoup(requests.get(url).text, "lxml")
soup = soup.find("section" , {"id" : "next-episode-widget"})
next_episode_number = soup.find("div", {"class" : "header-wrap"}).text.strip().splitlines()[0].strip()
next_eipsode_date = soup.find("div", {"class" : "header-wrap"}).find("h4").text.strip().split(";")[1].strip()
next_eipsode_day = soup.find("time", {"class" : "calendar-time icon left"}).find("em").text
print("next episode number:", next_episode_number)
print("next eipsode date  :", next_eipsode_date)
print("next eipsode day   :", next_eipsode_day)

import calendar
{month: index for index, month in enumerate(calendar.month_abbr) if month}[next_eipsode_date.split()[0]]
next_eipsode_date = next_eipsode_date.replace(next_eipsode_date.split()[0], str({month: index for index, month in enumerate(calendar.month_abbr) if month}[next_eipsode_date.split()[0]]))
next_eipsode_date = next_eipsode_date.replace(",", "")

from datetime import date
f = [int(i) for i in next_eipsode_date.split()]
print(f"Countdown          : {(date(year = f[-1], month = f[0], day = f[1]) - date.today()).days} days")

# file_name = "/home/amir/.selcul_next_episode_number.txt"
next_episode = int(next_episode_number.split(".")[0])

# if os.path.exists(file_name):
# 	if  open(file_name, "r").read().strip() == ne:
# 		print("\n\nLast episode watched!!!, wait for next episode\n\n")
# 		sys.exit()
# else:
# 	open(file_name, "w").write(ne)

print("""\n\n#############################################################
####-----------------""", end="")

download_ = "/home/amir/.selcul_last_downloaded.txt"
if os.path.exists(download_):
	last_downloaded_episode = int(open(download_, "r").read().strip())
	if next_episode - last_downloaded_episode > 1:
		print(colored(" CONGRATULATIONS", 'red'), end="")
	else:
		print(colored("--- NO NEW VIDEO", 'red'), end="")

print(""" -------------------####
#############################################################\n""")


print("Last downloaded    :", last_downloaded_episode)

last_episode_link = f"https://www.multipointtv.net/uyanis-buyuk-selcuklu-episode-{last_downloaded_episode}-english"
print("Last episode link  :", last_episode_link)
print("\n")
command = f"curl -s {last_episode_link} > /home/amir/ab__.txT"
os.system(command)

b = BeautifulSoup(open("/home/amir/ab__.txT", "r"), "lxml")
for i in b.select("iframe"):
	try:
		l = i['src']
		if "autoplay" in l:
			l = "https:" + l[:l.index("?")]
			print("Last episode Video Download link:", l)
	except:
		pass
os.remove("/home/amir/ab__.txT")


print("""
#############################################
# agar video download kar len to            #
# </home/amir/.selcul_last_downloaded.txt>  #
# file ko update kar den                    #
#############################################
""")

print()
